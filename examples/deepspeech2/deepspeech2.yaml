TrainingConfig:
    epochs: 70
    batch_size: 64 # maximum_batch_size=128, Otherwise, it is prone to memory errors.
    train_manifest: './train/libri_train_manifest.json' # Run librispeech_prepare.py, will generate xx_manifest.json

SpectConfig:
    sample_rate: 16000
    window_size: 0.02
    window_stride: 0.01
    window: "hamming"

AugmentationConfig:
    speed_volume_perturb: False
    spec_augment: False
    noise_dir: ''
    noise_prob: 0.4
    noise_min: 0.0
    noise_max: 0.5

ModelConfig:
    rnn_type: "LSTM"
    hidden_size: 1024
    hidden_layers: 5
    lookahead_context: 20

OptimConfig:
    learning_rate: 0.0003
    learning_anneal: 1.1
    weight_decay: 0.00001
    momentum: 0.9
    eps: 0.000000001
    betas: (0.9, 0.999)
    loss_scale: 1024
    epsilon: 0.00001

CheckpointConfig:
    ckpt_file_name_prefix: 'DeepSpeech'
    ckpt_path: './checkpoint'
    keep_checkpoint_max: 10

EvalConfig:
    batch_size: 128
    test_manifest: './eval/libri_test_clean_manifest.json'
    decoder_type: 'greedy'
    save_output: 'librispeech_val_output'

# use to finetune or eval model
Pretrained_model: ''

labels: ["'", "A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M",
         "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z", " ", "_"]
