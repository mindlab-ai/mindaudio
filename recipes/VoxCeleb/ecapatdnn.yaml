# Basic parameters
output_folder: /home/abc000/data/data_aug/
save_folder: /home/abc000/data/data_aug/save/
feat_folder: /disk1/huangyu/data/feat_train/ # path to store single file train feature
feat_folder_merge: /disk1/huangyu/data/feat_train_merge/ # path to store merged train feature

# Data files
data_folder: /home/abc000/data/voxceleb12  # e.g. data folder
train_annotation: /home/abc000/data/data_aug/save/train.csv  # pre generated csv, regenerate if not exist
valid_annotation: /home/abc000/data/data_aug/save/dev.csv    # pre generated csv, regenerate if not exist

verification_file: https://www.robots.ox.ac.uk/~vgg/data/voxceleb/meta/veri_test2.txt

skip_prep: false  # if false, generate csv file

device_target: "Ascend"
device_id: 1
run_distribute: 0

# Training parameters
number_of_epochs: 1
train_batch_size: 32
sample_rate: 16000
sentence_len: 3.0 # seconds
shuffle: true
random_chunk: true

# Feature parameters
n_mels: 80
left_frames: 0
right_frames: 0
deltas: false

dataloader_options:
  batch_size: 32
  shuffle: true
  num_workers: 4

concat_augment: true

# train params
in_channels: 80                                                 # input channel size, same as the dim of fbank feature
channels: 1024                                                  # channel size of middle layer feature map
base_lrate: 0.000001                                            # base learning rate of cyclic LR
max_lrate: 0.0001                                               # max learning rate of cyclic LR
momentum: 0.95                                                  # weight decay for optimizer
weight_decay: 0.000002                                          # momentum for optimizer
num_epochs: 3                                                   # training epoch
minibatch_size: 192                                             # batch size
emb_size: 192                                                   # embedding dim
step_size: 65000                                                # steps to achieve max learning rate cyclic LR
class_num: 7205                                                 # speaker num pf voxceleb1&2
pre_trained: False                                              # if pre-trained model exist

train_data_path: "/disk2/huangyu/data/feat_train_merge/"        # path to fbank training data
keep_checkpoint_max: 20
checkpoint_path: "train_ecapa_vox2_full-2_664204.ckpt"          # path to pre-trained model
ckpt_save_dir: "./ckpt/"                                        # path to store train model


# Basic parameters
feat_eval_folder: /disk1/huangyu/data/feat_eval/ # dir to store eval feature
feat_norm_folder: /disk1/huangyu/data/feat_norm/ # dir to store norm feature
eval_data_folder: /home/abc000/data/voxceleb12/ # data folder
eval_output_folder: /home/abc000/data/data_aug_eval/
eval_save_folder: /home/abc000/data/data_aug_eval/save/
device: 'cpu'

verification_file_bleeched: /disk1/huangyu/data/feat_eval/veri_test_bleeched.txt

# csv files
train_data: /home/abc000/data/data_aug_eval/save/train.csv
enrol_data: /home/abc000/data/data_aug_eval/save/enrol.csv
test_data: /home/abc000/data/data_aug_eval/save/test.csv

eval_batch_size: 1
score_norm: 's-norm'
n_train_snts: 400000 # used for normalization stats


# eval
eval_data_path: "/disk0/huangyu/data/feat_eval/"                  # path to eval fbank data
veri_file_path: "/disk0/huangyu/data/feat_eval/veri_test_bleeched.txt"      # trials
cut_wav: false                                                    # cut wav to 3s (cut wav to 3s, same as train data)
model_path: "/disk0/huangyu/data/ckpt/train_ecapa_vox12-2_6396.ckpt"    # path of eval model
train_norm_path: "/disk0/huangyu/data/feat_norm/"                         # fbank data for norm
cohort_size: 20000                                                # max number of utts to do norm
npy_file_path: '/disk0/huangyu/data/npys/'                                # dir to save intermediate result



